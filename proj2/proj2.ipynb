{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humor Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n",
      "   id                                               text  is_humor  \\\n",
      "0   1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
      "1   2  A man inserted an advertisement in the classif...         1   \n",
      "2   3  How many men does it take to open a can of bee...         1   \n",
      "3   4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
      "4   5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
      "\n",
      "   humor_rating  humor_controversy  offense_rating  \n",
      "0          2.42                1.0             0.2  \n",
      "1          2.50                1.0             1.1  \n",
      "2          1.95                0.0             2.4  \n",
      "3          2.11                1.0             0.0  \n",
      "4          2.78                0.0             0.1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import PunktSentenceTokenizer # unsupervised machine learning sentence tokenizer , can be trained, but we use the default\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Importing dataset\n",
    "dataset = pd.read_csv('files/train.csv', nrows=1000)\n",
    "print(dataset.shape)\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n",
      "   id                                               text  is_humor  \\\n",
      "0   1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
      "1   2  A man inserted an advertisement in the classif...         1   \n",
      "2   3  How many men does it take to open a can of bee...         1   \n",
      "3   4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
      "4   5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
      "\n",
      "   humor_rating  humor_controversy  \n",
      "0          2.42                1.0  \n",
      "1          2.50                1.0  \n",
      "2          1.95                0.0  \n",
      "3          2.11                1.0  \n",
      "4          2.78                0.0  \n",
      "\n",
      "----- Describe ------\n",
      "                id     is_humor  humor_rating  humor_controversy\n",
      "count  1000.000000  1000.000000    616.000000         616.000000\n",
      "mean    500.500000     0.616000      2.263393           0.517857\n",
      "std     288.819436     0.486601      0.571492           0.500087\n",
      "min       1.000000     0.000000      0.100000           0.000000\n",
      "25%     250.750000     0.000000      1.865000           0.000000\n",
      "50%     500.500000     1.000000      2.300000           1.000000\n",
      "75%     750.250000     1.000000      2.650000           1.000000\n",
      "max    1000.000000     1.000000      3.850000           1.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# selecting important columns only\n",
    "# dropping 'offense_rating'\n",
    "dataset = dataset[['id', 'text', 'is_humor', 'humor_rating', 'humor_controversy']]\n",
    "print(dataset.shape)\n",
    "print(dataset.head())\n",
    "\n",
    "print(\"\\n----- Describe ------\")\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     0\n",
       "text                   0\n",
       "is_humor               0\n",
       "humor_rating         384\n",
       "humor_controversy    384\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks if there are null values\n",
    "\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we have to process these null values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>'Trabajo,' the Spanish word for work, comes fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>I enrolled on some skill training and extra cu...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Men who ejaculated 21 times or more a month ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>I got REALLY angry today and it wasn't about n...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>A dog in Mexico named Frida saved the lives of...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  is_humor  \\\n",
       "5    6  'Trabajo,' the Spanish word for work, comes fr...         0   \n",
       "6    7  I enrolled on some skill training and extra cu...         0   \n",
       "8    9  Men who ejaculated 21 times or more a month ha...         0   \n",
       "9   10  I got REALLY angry today and it wasn't about n...         0   \n",
       "10  11  A dog in Mexico named Frida saved the lives of...         0   \n",
       "\n",
       "    humor_rating  humor_controversy  \n",
       "5            NaN                NaN  \n",
       "6            NaN                NaN  \n",
       "8            NaN                NaN  \n",
       "9            NaN                NaN  \n",
       "10           NaN                NaN  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['is_humor']==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 5)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['is_humor']==0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of null values is equal to the rows classifying the text as not humor, dropping these lines would drop important data for the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     int64\n",
       "text                  object\n",
       "is_humor               int64\n",
       "humor_rating         float64\n",
       "humor_controversy    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's done! Now some graphics to analyse text statictics... (in progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_list =[]\n",
    "data_list2 =[]\n",
    "\n",
    "raw_text =\"\"\n",
    "stop_words = tuple(set(stopwords.words(\"english\")))\n",
    "\n",
    "def lemmatization(token_sentence):\n",
    "    res = []\n",
    "    for word in token_sentence:\n",
    "        lemmatizer.lemmatize(word)\n",
    "        res.append(word)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# Simple test\n",
    "def input_test(classifier):\n",
    "    sample_text = input(\"Enter joke: \")\n",
    "    sample_text = re.sub('[^a-zA-Z]', ' ', sample_text).lower().split()\n",
    "    sample_text = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in sample_text])\n",
    "    X = vectorizer.transform([sample_text]).toarray()\n",
    "\n",
    "    print(X.shape)\n",
    "    print(X)\n",
    "\n",
    "    if(classifier.predict(X) == [1]):\n",
    "        print(\"It's a joke! (+)\")\n",
    "    else:\n",
    "        print(\"It's not a joke! (-)\")\n",
    "\n",
    "# range = dataset range\n",
    "for i in range(0,dataset.shape[0]):\n",
    "    \n",
    "   humor_sentence = dataset['text'][i]\n",
    "\n",
    "   #Tokenization by words or sentences\n",
    "   humor_sentence = word_tokenize(humor_sentence)\n",
    "   #remove stopwords and lemmatization with pos_tag\n",
    "   humor_sentence = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in humor_sentence if word not in set(stopwords.words('english'))])  \n",
    "  \n",
    "      \n",
    "   # remove alpha chars/not alphabetical caracters \n",
    "   humor_sentence2 = re.sub('[^a-zA-Z]', ' ', dataset['text'][i])\n",
    "   \n",
    "   humor_sentence2 = humor_sentence2.lower()\n",
    "   humor_sentence2 = word_tokenize(humor_sentence2)\n",
    "   humor_sentence2 = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in humor_sentence2 if word not in set(stopwords.words('english'))])  \n",
    "      \n",
    "   #raw_text += aux_reader+ \"\\n\"\n",
    "   \n",
    "    \n",
    "   # aggregate all the rows of the dataset in one corpus ('data_list')\n",
    "   data_list.append(humor_sentence)\n",
    "    \n",
    "    # how data_list2 is different from data_list?\n",
    "   data_list2.append(humor_sentence2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '100', '1000', '100ft', '106', '12', '1200', '1313', '14', '15', '18', '1861', '1940', '1948', '1984', '1st', '20', '2000', '2018', '2052', '2125', '22', '25', '30', '31', '3629', '365', '3d', '3rd', '40', '40th', '45', '48', '50', '500', '666', '70', '72', '75', '80', '85', '911', '999', 'aaaaafddasfrwe', 'abc', 'able', 'aboard', 'abroad', 'absent', 'absolutely', 'abuse', 'abuzz', 'accident', 'accidentally', 'accord', 'account', 'accountable', 'accuse', 'accuses', 'acquire', 'across', 'act', 'activity', 'actual', 'actually', 'ad', 'add', 'addict', 'address', 'adjust', 'admit', 'adopt', 'adult', 'adversity', 'advertisement', 'advice', 'aed', 'af', 'affect', 'afford', 'afraid', 'africa', 'african', 'after', 'age', 'agency', 'ago', 'ahead', 'ai', 'aid', 'aids', 'air', 'airline', 'airplane', 'airpod', 'ajar', 'album', 'alcohol', 'ale', 'alert', 'algae', 'alive', 'all', 'allah', 'allow', 'alone', 'alpha', 'already', 'alright', 'also', 'always', 'alzheimer', 'am', 'ama', 'amaze', 'amazon', 'ambition', 'america', 'american', 'americans', 'amount', 'amputee', 'an', 'anal', 'analogy', 'anatomy', 'ancient', 'and', 'angels', 'angry', 'animal', 'ankle', 'anniversary', 'announce', 'annoy', 'annoying', 'another', 'answer', 'ant', 'antifreeze', 'any', 'anybody', 'anycockledoooooo', 'anymore', 'anyone', 'anything', 'anywhere', 'apart', 'apnea', 'apologizes', 'app', 'apparently', 'apply', 'appreciate', 'arabs', 'are', 'argue', 'argument', 'aristotle', 'arkansas', 'arm', 'armageddon', 'around', 'arrangement', 'arrest', 'art', 'article', 'artisanal', 'artist', 'as', 'asian', 'aside', 'ask', 'asks', 'asleep', 'ass', 'assassinate', 'astronaut', 'astronomical', 'astros', 'asylum', 'at', 'ate', 'atom', 'atomic', 'attack', 'attenborough', 'attention', 'attitude', 'attractive', 'auctioneer', 'auntie', 'autocorrected', 'available', 'away', 'awesome', 'awkward', 'awww', 'ba', 'baby', 'back', 'backup', 'backwards', 'bad', 'badlands', 'bag', 'baghdad', 'bagpipe', 'balance', 'ball', 'bambi', 'ban', 'banana', 'band', 'bangkok', 'bank', 'baptize', 'bar', 'barack', 'barbecue', 'barber', 'barbie', 'barely', 'barman', 'barn', 'barrel', 'based', 'basically', 'basket', 'basketball', 'bath', 'bathing', 'bathroom', 'battery', 'bbq', 'be', 'beach', 'beat', 'beater', 'beatles', 'beautiful', 'because', 'become', 'becomes', 'bed', 'bedroom', 'bee', 'beef', 'beep', 'beer', 'bees', 'beethoven', 'before', 'begin', 'behavior', 'believe', 'belly', 'belt', 'bench', 'benefit', 'benign', 'best', 'bet', 'betrayed', 'between', 'bible', 'big', 'bike', 'biker', 'bill', 'bird', 'birdbath', 'birds', 'birth', 'birthday', 'bit', 'bite', 'biting', 'black', 'blacken', 'blacksmith', 'bladder', 'blast', 'bless', 'blessing', 'blimey', 'blind', 'blindfold', 'blink', 'blissfully', 'bloat', 'block', 'blond', 'blonde', 'blood', 'blow', 'blowjob', 'blue', 'blueberry', 'board', 'bob', 'body', 'boil', 'bomb', 'bond', 'bondage', 'boob', 'book', 'boom', 'boomboxes', 'boost', 'boot', 'booty', 'boring', 'born', 'bos', 'boss', 'bother', 'bottle', 'bottom', 'bought', 'bounce', 'bouquet', 'bowl', 'box', 'boy', 'boyardee', 'boyfriend', 'brad', 'brain', 'branch', 'brand', 'bread', 'break', 'breakfast', 'breaking', 'breast', 'breath', 'bride', 'bridesmaid', 'brie', 'bring', 'brings', 'brisk', 'bro', 'broad', 'brochure', 'broken', 'broom', 'brothel', 'brother', 'brought', 'brown', 'browser', 'bruise', 'buck', 'buff', 'bug', 'build', 'building', 'bulb', 'bullet', 'bullshit', 'bunch', 'burger', 'burglar', 'burgle', 'bury', 'bus', 'bushy', 'business', 'bust', 'busy', 'but', 'butterfly', 'button', 'buy', 'buying', 'buzz', 'buzzer', 'ca', 'cake', 'california', 'call', 'calmly', 'came', 'can', 'canada', 'canadian', 'cancer', 'cancerous', 'candidate', 'candy', 'cane', 'cannibal', 'cant', 'capitol', 'captain', 'caption', 'capture', 'car', 'carbon', 'card', 'cardboard', 'care', 'career', 'carpet', 'carry', 'cash', 'cashier', 'casino', 'cassette', 'cat', 'catch', 'category', 'catholics', 'catwoman', 'cauc', 'caucasian', 'caught', 'cause', 'caveman', 'cellphone', 'centimeter', 'certain', 'certainly', 'chair', 'champagne', 'chance', 'change', 'charade', 'charge', 'charity', 'charles', 'chase', 'cheat', 'check', 'cheek', 'cheese', 'cheetah', 'chef', 'chest', 'chick', 'chicken', 'child', 'chimney', 'chin', 'chinatown', 'chinese', 'ching', 'chino', 'chirp', 'chocolate', 'choir', 'choose', 'chooses', 'chop', 'christ', 'christian', 'christians', 'christmas', 'chromatica', 'chuck', 'church', 'churchitoes', 'cigarette', 'circle', 'circumcisor', 'city', 'civil', 'class', 'classified', 'claus', 'clean', 'cleaner', 'clearer', 'clearly', 'cliche', 'climate', 'climbed', 'clinic', 'clinton', 'close', 'clothes', 'cloud', 'clown', 'club', 'cnn', 'cob', 'cocaine', 'cock', 'code', 'coffee', 'colbert', 'cold', 'collection', 'college', 'color', 'colorado', 'comcast', 'come', 'comfortable', 'comic', 'common', 'communion', 'community', 'company', 'compare', 'competition', 'complain', 'completely', 'compliment', 'comprehend', 'compress', 'compromise', 'computer', 'condescend', 'condolence', 'condom', 'cone', 'confront', 'confuse', 'connector', 'consent', 'consider', 'constantly', 'constipate', 'constipated', 'contact', 'contains', 'contemplate', 'contestant', 'continually', 'control', 'conversation', 'cooking', 'cool', 'cooler', 'cooter', 'cooterpie', 'cootie', 'cop', 'corn', 'cornea', 'corny', 'corrects', 'correlation', 'cost', 'costco', 'costume', 'couch', 'could', 'counsel', 'country', 'couple', 'course', 'courteous', 'cousin', 'cow', 'cowboy', 'coworker', 'coworkers', 'crack', 'cracker', 'crayon', 'crazy', 'cream', 'create', 'credit', 'creepier', 'cremation', 'crescent', 'crime', 'crisis', 'criticism', 'crocs', 'croix', 'cross', 'crossfit', 'crow', 'cruise', 'crush', 'crust', 'cry', 'cube', 'cubic', 'cult', 'cum', 'cunt', 'cup', 'cupcake', 'curb', 'currently', 'curry', 'curtain', 'customer', 'cut', 'cute', 'cyclops', 'da', 'dad', 'daddy', 'dads', 'dago', 'dalai', 'damn', 'dan', 'dance', 'dancing', 'dangerous', 'dark', 'darwin', 'date', 'daughter', 'david', 'day', 'de', 'dead', 'deaf', 'deal', 'dealer', 'dealership', 'death', 'deathbed', 'decade', 'decapitate', 'decide', 'decision', 'declare', 'deer', 'defendant', 'defenseless', 'defines', 'definitely', 'delete', 'delicious', 'delight', 'deliver', 'delivery', 'delusion', 'demons', 'densest', 'denver', 'deodorant', 'department', 'depression', 'derail', 'derby', 'describe', 'deserves', 'designer', 'desire', 'despise', 'dessert', 'destruction', 'detail', 'detective', 'detector', 'developed', 'develops', 'devil', 'diagnose', 'dial', 'dialogue', 'diary', 'dick', 'did', 'die', 'difference', 'different', 'difficult', 'dildo', 'dinner', 'dinosaur', 'direction', 'director', 'dirty', 'disappoint', 'disappointed', 'disappointedly', 'discriminate', 'disguise', 'disgust', 'disk', 'disobedience', 'disorient', 'distance', 'district', 'dive', 'divorce', 'dizzy', 'do', 'doc', 'dockers', 'doctor', 'documentation', 'dodgers', 'does', 'dog', 'doll', 'dollop', 'dolphin', 'donate', 'donkey', 'dont', 'donut', 'doomsday', 'door', 'doorbell', 'doorway', 'down', 'downs', 'downstairs', 'doyouhaveasoreass', 'dozen', 'dr', 'drag', 'dragon', 'dragons', 'drape', 'draw', 'dream', 'dreamt', 'dress', 'drift', 'drink', 'drinkable', 'drinker', 'drinking', 'drive', 'driver', 'driveway', 'drop', 'dropbox', 'drown', 'drug', 'drunk', 'dry', 'dryer', 'dsaf', 'dublin', 'duck', 'dude', 'due', 'dug', 'dumb', 'dumbledore', 'during', 'dustpan', 'duty', 'dwarf', 'dwayne', 'dy', 'dynamite', 'dynasty', 'ear', 'early', 'earth', 'easily', 'east', 'easy', 'eat', 'eating', 'eats', 'ebola', 'egg', 'eggo', 'egyptian', 'eight', 'eightheist', 'either', 'ejaculate', 'ejaculation', 'election', 'electronics', 'elevator', 'elevennessee', 'else', 'email', 'embarrass', 'emos', 'emoticon', 'emotion', 'employee', 'empty', 'enchant', 'encouraged', 'end', 'english', 'enjoy', 'enough', 'enter', 'enters', 'entire', 'entirety', 'envelope', 'envy', 'epileptic', 'epileptikjlfasd', 'episode', 'equal', 'equally', 'erection', 'escape', 'esteem', 'etc', 'ethiopian', 'european', 'even', 'evening', 'event', 'eventually', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everytime', 'everywhere', 'evidence', 'eww', 'exactly', 'example', 'excellent', 'except', 'excite', 'exhaust', 'exit', 'exorcise', 'expect', 'experience', 'expert', 'explode', 'express', 'extremely', 'eye', 'eyeball', 'eyebrow', 'eyelash', 'face', 'facebook', 'facepalm', 'fact', 'factory', 'fairly', 'fairy', 'fake', 'fall', 'falon', 'falsely', 'family', 'fan', 'fantastic', 'far', 'farfrompoopin', 'fart', 'farting', 'fast', 'faster', 'fat', 'fatal', 'father', 'fault', 'favor', 'favorite', 'favourite', 'fedex', 'feed', 'feel', 'feeling', 'felt', 'female', 'feminist', 'ff', 'fiber', 'fiddle', 'field', 'fifth', 'fight', 'figure', 'fil', 'fill', 'film', 'finally', 'find', 'fine', 'finger', 'finish', 'fire', 'first', 'fish', 'fishing', 'fit', 'fitness', 'five', 'fix', 'flash', 'flasher', 'flat', 'flatter', 'flattery', 'flick', 'flint', 'flip', 'flirt', 'float', 'flock', 'floor', 'floss', 'flower', 'fluid', 'fly', 'flying', 'follow', 'follower', 'food', 'foot', 'footprint', 'for', 'force', 'forest', 'forever', 'forget', 'forgot', 'form', 'formally', 'fort', 'forty', 'found', 'four', 'fox', 'frame', 'frank', 'free', 'freedom', 'freezer', 'french', 'fridge', 'friend', 'friggin', 'frittata', 'from', 'front', 'fry', 'fuck', 'fuckin', 'fucking', 'fulfil', 'fulfill', 'full', 'fun', 'fundamental', 'funeral', 'fungimentalist', 'funny', 'furniture', 'furry', 'fury', 'gala', 'gallon', 'game', 'gang', 'gangster', 'gangthter', 'garbage', 'garden', 'gasp', 'gay', 'geek', 'generic', 'genie', 'george', 'gerd', 'german', 'germany', 'get', 'gf', 'ghost', 'gift', 'gim', 'ginger', 'gingerbread', 'girl', 'girlfriend', 'girls', 'give', 'glad', 'glance', 'glass', 'glorious', 'glove', 'go', 'god', 'goddamnit', 'golf', 'golfer', 'gomez', 'gon', 'good', 'goodbye', 'goodnight', 'goodyear', 'google', 'googled', 'got', 'government', 'gracias', 'gram', 'grandeur', 'grandma', 'grandmother', 'grandpa', 'grandparent', 'grandson', 'grapes', 'grateful', 'gravity', 'gravy', 'great', 'greece', 'greek', 'green', 'grenade', 'grey', 'grog', 'groom', 'gross', 'grouch', 'ground', 'group', 'grow', 'grown', 'grumpy', 'guaranteed', 'guess', 'guitar', 'gun', 'gut', 'guy', 'guys', 'gym', 'haaaaay', 'hack', 'haha', 'hair', 'hairdresser', 'half', 'halloween', 'hamster', 'hand', 'handsome', 'hannover', 'hans', 'happen', 'happens', 'happier', 'happy', 'harbor', 'hard', 'hardest', 'hardware', 'hardwood', 'hate', 'hater', 'hath', 'haunt', 'have', 'having', 'hawking', 'he', 'head', 'headache', 'headbanging', 'heading', 'headless', 'healthier', 'healthy', 'hear', 'heard', 'heart', 'heating', 'heaven', 'heavier', 'heavy', 'helen', 'hell', 'hellen', 'helluva', 'help', 'her', 'herb', 'here', 'herman', 'hero', 'heroin', 'hey', 'heyyyy', 'hiddleston', 'hide', 'hiding', 'hieroglyphic', 'high', 'hilarious', 'him', 'hippie', 'hipster', 'his', 'hispanic', 'history', 'hit', 'hitchhiker', 'ho', 'hobo', 'hold', 'hole', 'holidays', 'home', 'homeless', 'homolone', 'homophobic', 'homosexual', 'honey', 'hoo', 'horny', 'horoscope', 'horrible', 'horror', 'horse', 'hose', 'hosen', 'hospital', 'hot', 'hotel', 'hour', 'house', 'houseboat', 'how', 'hr', 'hug', 'huge', 'human', 'humanity', 'humans', 'hummingbird', 'hummusexual', 'humor', 'hun', 'hunger', 'hurt', 'husband', 'hyphen', 'ice', 'iced', 'icu', 'idea', 'idiot', 'idk', 'if', 'ignore', 'ihop', 'ii', 'ill', 'im', 'image', 'imaginary', 'imagine', 'imitation', 'immature', 'immediate', 'immediately', 'immunity', 'impersonate', 'important', 'impossible', 'impress', 'impressive', 'in', 'include', 'income', 'incredible', 'india', 'indian', 'infamous', 'infection', 'inflate', 'information', 'inhaler', 'inmate', 'inn', 'inner', 'inning', 'innocent', 'insane', 'insect', 'insert', 'inside', 'insist', 'inspirational', 'instagram', 'instant', 'instead', 'instinct', 'intend', 'intense', 'intensive', 'interested', 'intern', 'internet', 'interview', 'interviewed', 'interviewer', 'invent', 'inventive', 'inventor', 'invite', 'involuntary', 'iphone', 'iran', 'iraq', 'iraqed', 'ireland', 'irish', 'irishman', 'is', 'isis', 'islam', 'islamic', 'islamism', 'islams', 'island', 'it', 'italian', 'italy', 'its', 'iv', 'jagger', 'jail', 'james', 'january', 'javelin', 'jaws', 'jaywalking', 'jehovas', 'jerry', 'jersey', 'jesus', 'jew', 'jewish', 'jewpiter', 'jigglypuff', 'jihad', 'jim', 'jimbly', 'jimcarrey', 'jimmy', 'job', 'joe', 'johnson', 'join', 'joint', 'joke', 'joking', 'jong', 'jordan', 'journal', 'joy', 'judgement', 'judgmental', 'jump', 'junky', 'just', 'kale', 'karate', 'kayak', 'keep', 'keller', 'kentucky', 'kept', 'kettle', 'kfc', 'kick', 'kid', 'kids', 'kill', 'killer', 'killing', 'kilomockingbirds', 'kim', 'kind', 'kindergarten', 'king', 'kinky', 'kitchen', 'kitten', 'knee', 'knew', 'knife', 'knock', 'know', 'kong', 'korma', 'kosher', 'kraken', 'kraut', 'la', 'lab', 'labor', 'lack', 'ladder', 'lady', 'ladybug', 'laid', 'lama', 'laminate', 'land', 'landlord', 'language', 'large', 'laser', 'last', 'late', 'later', 'laugh', 'laughing', 'laughter', 'laundry', 'law', 'lawn', 'lawsuit', 'lay', 'lazy', 'lead', 'leader', 'leaf', 'leak', 'learn', 'least', 'leave', 'lee', 'left', 'leftover', 'leg', 'legal', 'legged', 'leper', 'lesbian', 'less', 'let', 'letter', 'lettuce', 'level', 'librarian', 'lick', 'lie', 'life', 'lifelike', 'lifter', 'light', 'lightbulb', 'lighthearted', 'lightning', 'like', 'linda', 'line', 'lion', 'liquid', 'lisdexic', 'listen', 'litre', 'little', 'live', 'livestock', 'living', 'll', 'lmao', 'local', 'lock', 'lol', 'lonely', 'long', 'longer', 'look', 'lookin', 'looks', 'lord', 'lordin', 'lose', 'lot', 'lotion', 'lottery', 'loud', 'love', 'lovely', 'low', 'loyal', 'luck', 'lucky', 'lumberjack', 'lump', 'lunch', 'lyric', 'ma', 'machine', 'mad', 'magazine', 'magic', 'magical', 'mail', 'main', 'make', 'maker', 'makeup', 'making', 'male', 'malt', 'mama', 'man', 'manage', 'mandalorian', 'manly', 'mannequin', 'many', 'map', 'mar', 'march', 'marijuana', 'marketing', 'marma', 'marriage', 'married', 'marry', 'mars', 'mash', 'mashed', 'mask', 'mass', 'massage', 'masseuse', 'master', 'masturbation', 'mat', 'match', 'mate', 'material', 'mating', 'mattel', 'matthew', 'may', 'maybe', 'mcdonalds', 'mcmaster', 'md', 'me', 'meal', 'mean', 'medium', 'meet', 'mega', 'melville', 'member', 'memoir', 'men', 'meow', 'meowntain', 'mess', 'message', 'met', 'meth', 'mething', 'meundies', 'mexican', 'mexicans', 'mexico', 'mia', 'michael', 'michelle', 'michigan', 'mick', 'middle', 'midget', 'might', 'mike', 'military', 'milk', 'millenials', 'millennials', 'million', 'millions', 'min', 'mind', 'mindalorian', 'mindy', 'mine', 'mineral', 'minor', 'mint', 'minute', 'miracle', 'mirror', 'missouri', 'mistake', 'mister', 'mistook', 'misunderstood', 'mmmm', 'mobility', 'mockingbird', 'mole', 'mom', 'moma', 'momma', 'monday', 'money', 'monkey', 'monopoly', 'monoxide', 'monster', 'month', 'moon', 'mormon', 'mormons', 'morning', 'moron', 'morse', 'mosquito', 'most', 'mostly', 'mother', 'mouth', 'move', 'movie', 'mow', 'mower', 'mrs', 'ms', 'much', 'muffler', 'mugger', 'mugshot', 'multiculturalism', 'mumbai', 'mural', 'murder', 'muscle', 'mushroom', 'muslim', 'must', 'my', 'mysterious', 'na', 'nah', 'nail', 'naked', 'name', 'narnia', 'naturally', 'nazi', 'need', 'needle', 'negative', 'neighbor', 'neighborhood', 'neither', 'nerd', 'nerve', 'nervous', 'nest', 'netherlands', 'never', 'new', 'news', 'newspaper', 'next', 'nice', 'nickname', 'nigeria', 'nigga', 'night', 'ninja', 'ninjavitis', 'no', 'nobel', 'nobody', 'nod', 'non', 'noncommittal', 'none', 'normal', 'north', 'not', 'note', 'nothing', 'notice', 'nougat', 'now', 'nuclear', 'number', 'numeral', 'numerous', 'nun', 'nut', 'obama', 'obese', 'obsession', 'ocd', 'ocean', 'october', 'of', 'offend', 'offensive', 'office', 'officer', 'officially', 'often', 'oh', 'ok', 'okay', 'old', 'olympics', 'on', 'once', 'one', 'online', 'only', 'oooh', 'open', 'opening', 'oppose', 'optimist', 'or', 'orange', 'orbison', 'order', 'org', 'organise', 'orgasm', 'original', 'ornithologist', 'orphan', 'orwell', 'osmium', 'other', 'others', 'out', 'outside', 'outstanding', 'oven', 'overcome', 'overhear', 'overweight', 'overwhelm', 'overwhelmed', 'own', 'owner', 'oxygen', 'pacific', 'pack', 'package', 'packed', 'paddy', 'page', 'pageant', 'pager', 'paid', 'pain', 'pakistan', 'pallbearer', 'palm', 'pamphlet', 'panda', 'pang', 'panic', 'pant', 'paper', 'parent', 'park', 'parlor', 'parody', 'part', 'partial', 'partially', 'partner', 'party', 'pas', 'pass', 'passion', 'past', 'pasta', 'pasteurise', 'patriotic', 'pave', 'paw', 'pay', 'pea', 'peace', 'peek', 'pencil', 'penis', 'people', 'pepper', 'perfume', 'period', 'peroxide', 'person', 'personal', 'personality', 'pet', 'petal', 'petit', 'petrol', 'pharaoh', 'pharmacy', 'phone', 'photo', 'photographer', 'phuket', 'physic', 'pic', 'pick', 'picked', 'picture', 'pie', 'piece', 'pigeon', 'piggyback', 'pile', 'pillow', 'pilot', 'pin', 'pinch', 'pine', 'ping', 'pinocchio', 'pint', 'pirate', 'piss', 'pitch', 'pits', 'pitt', 'pizza', 'place', 'plane', 'planet', 'plant', 'plastic', 'plato', 'play', 'pleasant', 'please', 'pleasure', 'plot', 'plow', 'poacher', 'pod', 'podcast', 'podcasts', 'point', 'poker', 'pole', 'police', 'policeman', 'policewoman', 'polish', 'political', 'politics', 'poofreader', 'pool', 'poor', 'poorly', 'pop', 'popular', 'population', 'porch', 'portable', 'pose', 'position', 'possibly', 'post', 'potato', 'pour', 'powder', 'power', 'practice', 'predict', 'pregnancy', 'pregnant', 'premarital', 'premature', 'prepare', 'preserve', 'preserver', 'president', 'presidential', 'press', 'pressure', 'pretend', 'pretty', 'pretzel', 'price', 'pride', 'priest', 'primarily', 'prince', 'prison', 'prisoner', 'private', 'prize', 'probably', 'problem', 'procrastinate', 'produce', 'professor', 'promise', 'promo', 'promptly', 'pronounce', 'proper', 'properly', 'prostitute', 'protip', 'proven', 'psycho', 'pub', 'pubes', 'public', 'puddle', 'puked', 'pull', 'pulp', 'punch', 'punishment', 'pupil', 'pursue', 'purty', 'push', 'pussy', 'put', 'pyramid', 'quality', 'quarter', 'que', 'queer', 'question', 'quiche', 'quick', 'quickly', 'quiet', 'quilted', 'quilty', 'quinoa', 'quit', 'quite', 'quote', 'qué', 'rabbi', 'race', 'racey', 'racist', 'raconteurs', 'radical', 'radio', 'rag', 'railroad', 'rain', 'ran', 'rang', 'rare', 'rather', 'ravi', 'ray', 'razor', 're', 'reach', 'read', 'reader', 'reading', 'ready', 'real', 'realistic', 'realize', 'really', 'reaper', 'reason', 'rebuild', 'recap', 'recede', 'receive', 'recently', 'recipe', 'reclassify', 'recommend', 'record', 'red', 'redhead', 'reduce', 'refuse', 'register', 'regret', 'regular', 'reindeer', 'relation', 'relationship', 'release', 'relief', 'religion', 'religious', 'remember', 'remind', 'reminder', 'remove', 'rent', 'reply', 'reporter', 'republican', 'respond', 'responds', 'rest', 'restaurant', 'result', 'retire', 'return', 'reunite', 'reverse', 'revolve', 'rhode', 'rhyme', 'rick', 'rid', 'ride', 'right', 'rim', 'ring', 'riot', 'rip', 'ripped', 'rise', 'road', 'robber', 'roberto', 'robot', 'rode', 'rofl', 'rohypnol', 'roll', 'rolled', 'rollerblade', 'roman', 'romania', 'romps', 'room', 'rooster', 'root', 'rope', 'roses', 'rough', 'rover', 'row', 'roy', 'rubber', 'ruin', 'rule', 'run', 'sack', 'sad', 'sadly', 'safe', 'safety', 'salad', 'saltnpeppa', 'salty', 'same', 'sample', 'sand', 'sandwich', 'sandy', 'santa', 'sapien', 'sash', 'sauce', 'saunter', 'saving', 'saw', 'say', 'saying', 'says', 'scar', 'scarecrow', 'scene', 'scheme', 'school', 'science', 'scientology', 'scrabble', 'scratch', 'screech', 'screw', 'scrub', 'sea', 'seashell', 'season', 'seater', 'second', 'secret', 'secretly', 'see', 'seeing', 'seem', 'select', 'selena', 'self', 'selfie', 'sell', 'sellout', 'send', 'sense', 'sentence', 'september', 'serial', 'series', 'serious', 'service', 'sesame', 'set', 'settle', 'seven', 'several', 'sewage', 'sex', 'sexyphone', 'shade', 'shady', 'shake', 'shampoo', 'shape', 'share', 'shark', 'sharona', 'sharp', 'sharpie', 'shave', 'she', 'shea', 'sheepishly', 'shell', 'sherlock', 'ship', 'shirt', 'shit', 'shitfaced', 'shitpotle', 'shoe', 'shoebox', 'shooed', 'shoot', 'shop', 'shopping', 'shot', 'shoulder', 'shout', 'shovel', 'show', 'shower', 'shrink', 'shut', 'sick', 'side', 'sigh', 'sign', 'signal', 'signing', 'silence', 'silencer', 'silent', 'silently', 'silly', 'sillyantro', 'simple', 'simply', 'since', 'sincerest', 'sing', 'singapore', 'singer', 'singh', 'single', 'sir', 'siri', 'sister', 'sit', 'sitars', 'sitting', 'situation', 'six', 'size', 'skateboard', 'skeletons', 'skiing', 'skin', 'skinny', 'skull', 'slave', 'slavery', 'sleep', 'slice', 'slightly', 'slim', 'slimy', 'slip', 'small', 'smell', 'smile', 'smiley', 'smoke', 'smoking', 'smuggle', 'snail', 'snake', 'snapper', 'snatch', 'sneak', 'sneaker', 'sneeze', 'snort', 'snow', 'snowblower', 'snowblowing', 'snowman', 'so', 'sob', 'social', 'soda', 'sofa', 'sofishticated', 'soften', 'soldier', 'some', 'somebody', 'someone', 'something', 'sometimes', 'son', 'song', 'soon', 'sore', 'sorry', 'sound', 'soup', 'sour', 'south', 'space', 'spacecraft', 'spaceship', 'spacesuit', 'spaghetti', 'spam', 'spanish', 'spark', 'speak', 'speaker', 'spear', 'special', 'specific', 'specimen', 'speech', 'speed', 'spell', 'spellcheck', 'spelling', 'spend', 'spending', 'spent', 'sperm', 'spider', 'spiderweb', 'spin', 'split', 'spoil', 'spoiler', 'spokesman', 'sport', 'spot', 'spouse', 'spray', 'spread', 'spring', 'squad', 'square', 'stab', 'stair', 'stairs', 'stall', 'stand', 'star', 'stars', 'start', 'state', 'states', 'statistic', 'statistics', 'stay', 'steal', 'step', 'stephen', 'stereo', 'stick', 'still', 'stole', 'stones', 'stool', 'stop', 'store', 'storm', 'straight', 'stranger', 'strawberry', 'stream', 'street', 'strength', 'stress', 'strip', 'stroke', 'strong', 'strongly', 'struggle', 'stuck', 'study', 'stuff', 'stuffed', 'stupid', 'subatomic', 'subject', 'substance', 'subtly', 'subversive', 'subway', 'successful', 'suck', 'suddenly', 'sue', 'suggest', 'suicide', 'suit', 'summer', 'sun', 'sunday', 'sunglass', 'superman', 'support', 'sure', 'surface', 'surgeon', 'surgery', 'suspense', 'suspicion', 'swallow', 'swap', 'swear', 'swing', 'switch', 'swore', 'syndrome', 'system', 'tabacky', 'table', 'tablet', 'tac', 'taco', 'take', 'talented', 'taliban', 'talk', 'tall', 'tampax', 'tan', 'tandem', 'tank', 'tape', 'taste', 'tattoo', 'taught', 'taurus', 'tax', 'tea', 'teach', 'teacher', 'tear', 'tease', 'teen', 'teenage', 'teeth', 'teethbrush', 'tell', 'temperature', 'ten', 'tendency', 'tennessee', 'tennis', 'terminator', 'terraform', 'terrible', 'terribly', 'terrorist', 'test', 'texas', 'text', 'thai', 'thank', 'thanks', 'that', 'the', 'theater', 'their', 'then', 'therapist', 'therapy', 'there', 'thermostat', 'these', 'they', 'thicc', 'thigh', 'thin', 'thing', 'things', 'think', 'thinking', 'thirsty', 'this', 'those', 'though', 'thought', 'threat', 'threaten', 'three', 'threesome', 'thrones', 'throw', 'thrown', 'thunder', 'thunderstorm', 'thursday', 'tic', 'ticket', 'tide', 'tie', 'tiger', 'tight', 'till', 'tim', 'timbly', 'time', 'times', 'tin', 'tiny', 'tire', 'tit', 'title', 'to', 'toast', 'today', 'toddler', 'toe', 'together', 'toilet', 'told', 'tomorrow', 'tonigh', 'tonight', 'too', 'toodles', 'tooth', 'toothbrush', 'toothpaste', 'top', 'touch', 'tough', 'tour', 'towel', 'town', 'track', 'trade', 'tradition', 'traffic', 'trail', 'train', 'trampoline', 'tranny', 'transformer', 'transgender', 'transition', 'transvestite', 'treadmill', 'treasure', 'treat', 'tree', 'trick', 'trois', 'trouble', 'troubleshoot', 'trouser', 'truck', 'true', 'truly', 'trust', 'try', 'tuesday', 'tumor', 'tune', 'turkey', 'turn', 'turns', 'tv', 'tweet', 'twitter', 'two', 'ty', 'type', 'tyre', 'tyson', 'u2', 'ugliest', 'ugly', 'uh', 'uhh', 'um', 'ummm', 'un', 'unable', 'unbelievable', 'uncontrollable', 'undercover', 'underground', 'understand', 'unemployment', 'unfair', 'unfaithful', 'unfortunately', 'united', 'universe', 'unless', 'unmotivated', 'unsung', 'until', 'up', 'upload', 'upset', 'upside', 'ur', 'urge', 'urine', 'us', 'usb', 'use', 'user', 'vaccum', 'vacuum', 'valentine', 'vampire', 'vaping', 'vase', 'vasectomy', 've', 'vegan', 'vegetable', 'verdict', 'very', 'vet', 'veteran', 'vhs', 'vi', 'vibrate', 'video', 'vietnam', 'vile', 'virgin', 'virginia', 'vision', 'visit', 'visitation', 'vlad', 'volunteer', 'wacky', 'wafer', 'waffle', 'wait', 'waiter', 'wake', 'walk', 'walker', 'walkers', 'walks', 'wall', 'wallet', 'walmart', 'wan', 'want', 'wanted', 'war', 'warbirds', 'wardrobe', 'warm', 'wars', 'warsaw', 'was', 'wash', 'watch', 'watching', 'water', 'way', 'wayyyyy', 'we', 'weakness', 'weapon', 'wear', 'web', 'webster', 'wedding', 'weddings', 'week', 'weekend', 'weigh', 'weight', 'weird', 'welcome', 'well', 'wench', 'went', 'west', 'wetting', 'whale', 'what', 'whatever', 'whats', 'wheelchair', 'when', 'whenever', 'where', 'whereby', 'which', 'whisper', 'white', 'whiten', 'whiteness', 'who', 'whoa', 'whole', 'whorephans', 'whose', 'why', 'widdow', 'widow', 'wife', 'wild', 'wildfire', 'wildly', 'win', 'window', 'wine', 'wink', 'winter', 'wish', 'witch', 'with', 'without', 'witnesses', 'witty', 'wo', 'woke', 'woman', 'women', 'wonder', 'wong', 'wonka', 'wood', 'woodchuck', 'woods', 'wop', 'word', 'work', 'worker', 'working', 'workout', 'workplace', 'world', 'worried', 'worse', 'would', 'wow', 'wrestler', 'wrinkle', 'write', 'writer', 'writes', 'wrong', 'wtf', 'xanax', 'ya', 'yahweh', 'yeah', 'year', 'yeast', 'yell', 'yellow', 'yes', 'yesterday', 'yo', 'yoda', 'you', 'young', 'your', 'zero', 'zombie', 'zombified', 'zone', 'ㅤㅤ']\n",
      "(616, 2615) (616,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create bag-of-words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 8000)\n",
    "X = vectorizer.fit_transform(data_list).toarray() # no. of features per phrase (phrase=X[row])\n",
    "y = dataset.iloc[:,2].values\n",
    "\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(492, 2615) (492,)\n",
      "(124, 2615) (124,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def print_metrics(y_test, y_pred):\n",
    "    # confusion matrix\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # accuracy\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # precision\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "\n",
    "    # recall\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "\n",
    "    # f1\n",
    "    print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[124]]\n",
      "Accuracy:  1.0\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1:  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit Naive Bayes to the training set\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "print_metrics(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter joke: 'Trabajo,' the Spanish word for work, comes from the Latin term 'trepaliare,' meaning torture.\n",
      "(1, 2615)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "It's a joke! (+)\n"
     ]
    }
   ],
   "source": [
    "input_test(classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26  54]\n",
      " [  3 117]]\n",
      "Accuracy:  0.715\n",
      "Precision:  0.6842105263157895\n",
      "Recall:  0.975\n",
      "F1:  0.8041237113402062\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter joke: I once toasted the bride and groom at a Pakistani wedding. All I did was push the button on the drone\n",
      "(1, 3973)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "It's a joke!\n"
     ]
    }
   ],
   "source": [
    "input_test(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-c9cc3f01b2df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[0m\u001b[0;32m   1373\u001b[0m                              \u001b[1;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m                              \" class: %r\" % classes_[0])\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter joke: I once toasted the bride and groom at a Pakistani wedding. All I did was push the button on the drone\n",
      "(1, 3973)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "It's a joke!\n"
     ]
    }
   ],
   "source": [
    "input_test(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 79  40]\n",
      " [ 33 148]]\n",
      "Accuracy:  0.7566666666666667\n",
      "Precision:  0.7872340425531915\n",
      "Recall:  0.8176795580110497\n",
      "F1:  0.8021680216802167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier = MLPClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter joke: I once toasted the bride and groom at a Pakistani wedding. All I did was push the button on the drone\n",
      "(1, 3973)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "It's a joke!\n"
     ]
    }
   ],
   "source": [
    "input_test(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26  54]\n",
      " [  3 117]]\n",
      "Accuracy:  0.715\n",
      "Precision:  0.6842105263157895\n",
      "Recall:  0.975\n",
      "F1:  0.8041237113402062\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "lassifier = BernoulliRBM()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter joke: I once toasted the bride and groom at a Pakistani wedding. All I did was push the button on the drone\n",
      "(1, 3973)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "It's a joke! (+)\n"
     ]
    }
   ],
   "source": [
    "input_test(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
